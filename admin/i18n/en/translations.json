{
    "A descriptive name for your function": "A descriptive name for your function",
    "API Token": "API Token",
    "API Token for Inference Server": "API Token for Inference Server",
    "Active": "Active",
    "Ask": "Ask",
    "Assistant Settings": "Assistant Settings",
    "Assistant can use Object": "Assistant can use Object",
    "Assistant can use this function": "Assistant can use this function",
    "Custom functions for assistant": "Custom functions for assistant",
    "Datapoint (Request)": "Datapoint (Request)",
    "Datapoint (Result)": "Datapoint (Result)",
    "Debug / Chain-of-Thought Output": "Debug / Chain-of-Thought Output",
    "Define custom functions for the assistant. Make sure to add a good description for your functions so the assistant knows when to call your function. Each function needs a datapoint that starts the process and another datapoint that contains the result of your function.": "Define custom functions for the assistant. Make sure to add a good description for your functions so the assistant knows when to call your function. Each function needs a datapoint that starts the process and another datapoint that contains the result of your function.",
    "Describe the personality of your assistant": "Describe the personality of your assistant",
    "Describe what your function does and how the data for the request should look. This is important for the assistant to understand your function.": "Describe what your function does and how the data for the request should look. This is important for the assistant to understand your function.",
    "Description": "Description",
    "Do you really want to import objects from enum.rooms? Existing objects will be reset!": "Do you really want to import objects from enum.rooms? Existing objects will be reset!",
    "ERROR: column 'Model' must contain unique text": "ERROR: column 'Model' must contain unique text",
    "English": "English",
    "Friendly and helpful": "Friendly and helpful",
    "Functions": "Functions",
    "German": "German",
    "Give your personal assistant a name and describe its personality. Choose a model that should be used for your assistant.": "Give your personal assistant a name and describe its personality. Choose a model that should be used for your assistant.",
    "How long to wait between retries": "How long to wait between retries",
    "How many times should we retry if request to model fails": "How many times should we retry if request to model fails",
    "If greater 0 previous messages will be included in the request so the tool will stay in context": "If greater 0 previous messages will be included in the request so the tool will stay in context",
    "Import objects from enum.rooms": "Import objects from enum.rooms",
    "Language": "Language",
    "Last answer": "Last answer",
    "Limit the response of the tool to your desired amount of tokens.": "Limit the response of the tool to your desired amount of tokens.",
    "Link to LM Studio": "Link to LM Studio",
    "Link to LocalAI": "Link to LocalAI",
    "Max. Retries": "Max. Retries",
    "Max. Tokens": "Max. Tokens",
    "Message History (Chat Mode)": "Message History (Chat Mode)",
    "Model": "Model",
    "Model Settings": "Model Settings",
    "Model is active": "Model is active",
    "Models": "Models",
    "Name": "Name",
    "Name for the Assistant": "Name for the Assistant",
    "Name of the Model": "Name of the Model",
    "Object": "Object",
    "Object access for assistant": "Object access for assistant",
    "Objects": "Objects",
    "Personality": "Personality",
    "Please add the objects you want to use with the assistant. The assistant will be able to read and control these objects. You can use the button to import all states from your configured room sorting. Make sure to only include needed states to save tokens.": "Please add the objects you want to use with the assistant. The assistant will be able to read and control these objects. You can use the button to import all states from your configured room sorting. Make sure to only include needed states to save tokens.",
    "Please enter your Anthropic API Token to start using models like Opus, Haiku and Sonnet. If there are new models released you can simply add them in the table to start using them with ai assistants.": "Please enter your Anthropic API Token to start using models like Opus, Haiku and Sonnet. If there are new models released you can simply add them in the table to start using them with ai assistants.",
    "Please enter your Deepseek API Token to start using the models. If there are new models released you can simply add them in the table to start using them with ai assistants.": "Please enter your Deepseek API Token to start using the models. If there are new models released you can simply add them in the table to start using them with ai assistants.",
    "Please enter your OpenAI API Token to start using models like Gpt4, Gpt4-o1, Gpt3-5. If there are new models released you can simply add them in the table to start using them with ai assistants.": "Please enter your OpenAI API Token to start using models like Gpt4, Gpt4-o1, Gpt3-5. If there are new models released you can simply add them in the table to start using them with ai assistants.",
    "Please enter your Openrouter API Token to start using the models. If there are new models released you can simply add them in the table to start using them with ai assistants.": "Please enter your Openrouter API Token to start using the models. If there are new models released you can simply add them in the table to start using them with ai assistants.",
    "Please enter your Perplexity API Token to start using the models. If there are new models released you can simply add them in the table to start using them with ai assistants.": "Please enter your Perplexity API Token to start using the models. If there are new models released you can simply add them in the table to start using them with ai assistants.",
    "Question": "Question",
    "Request Settings": "Request Settings",
    "Retry Delay": "Retry Delay",
    "Room or sorting for Object": "Room or sorting for Object",
    "Save the configuration first": "To test the assistant, first be sure you saved the configuration",
    "Select how many messages should be included for context retention. Temperature defines creativity/randomness of output from 0-1 where 0 is the most predictable output. Set how many tokens should be generated max for assistant responses.": "Select how many messages should be included for context retention. Temperature defines creativity/randomness of output from 0-1 where 0 is the most predictable output. Set how many tokens should be generated max for assistant responses.",
    "Select if failed requests to the assistant should be retried and how long to wait between tries.": "Select if failed requests to the assistant should be retried and how long to wait between tries.",
    "Select the language that should be used by the assistant": "Select the language that should be used by the assistant",
    "Setting for creativity/consistency of the models response. (Leave at default if you are not sure!=": "Setting for creativity/consistency of the models response. (Leave at default if you are not sure!=",
    "Settings": "Settings",
    "Sort": "Sort",
    "Temperature": "Temperature",
    "The datapoint that contains the result of your function call (Has to be fulfilled in 60 Seconds!)": "The datapoint that contains the result of your function call (Has to be fulfilled in 60 Seconds!)",
    "The datapoint that starts the request for the function": "The datapoint that starts the request for the function",
    "URL for Inference Server": "URL for Inference Server",
    "When activated the internal thought process of the assistant will be written to the response datapoint": "When activated the internal thought process of the assistant will be written to the response datapoint",
    "Which Model should be used": "Which Model should be used",
    "You can use your custom or self hosted inference server to run open source models. The server needs to follow the rest api standards used by many providers, see examples below. Please make sure to add your used models by name to the table below.": "You can use your custom or self hosted inference server to run open source models. The server needs to follow the rest api standards used by many providers, see examples below. Please make sure to add your used models by name to the table below."
}